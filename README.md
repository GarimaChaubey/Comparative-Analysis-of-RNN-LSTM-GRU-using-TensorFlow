# ğŸ¤– Comparative Analysis of RNN, LSTM, and GRU using TensorFlow

This project provides a **beginner-friendly, side-by-side comparison** of three popular recurrent neural network architectures:  
**Simple RNN**, **LSTM**, and **GRU** for sequence prediction tasks.

The models are implemented using **TensorFlow/Keras**, and compared on the basis of training time, accuracy, convergence, and stability over longer sequences.

---

## ğŸ“ Files Included

- `RNN_Assignment.ipynb` â€” Full implementation of RNN, LSTM, and GRU models
- `README.md` â€” Project documentation and insights

---

## ğŸ¯ Objective

To understand and compare the performance of different types of recurrent neural networks:

- âœ… How quickly they train
- ğŸ“‰ How fast they converge
- ğŸ¯ How accurate they are on unseen data
- ğŸ” How well they handle long sequences

---

## ğŸ§  Dataset

This project uses a **simple sequence prediction dataset** (you can replace it with IMDB, weather data, or phoneme datasets).

### Preprocessing steps include:
- Tokenization of sequences
- Padding to handle varying lengths
- One-hot encoding for labels
- Train-test split

---

## ğŸ— Model Architectures

Each model contains:
- ğŸ“¥ Input Layer (Embedding)
- ğŸ” One Recurrent Layer:
  - RNN / LSTM / GRU
- ğŸ¯ Dense Output Layer with `sigmoid` activation

---

## ğŸš€ How to Run

### 1. Clone the repository

```bash
git clone https://github.com/<your-username>/Comparative-Analysis-of-RNN-LSTM-GRU.git
cd Comparative-Analysis-of-RNN-LSTM-GRU
